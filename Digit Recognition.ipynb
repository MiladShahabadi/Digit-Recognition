{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xz_ECu1kmgHU"
   },
   "source": [
    "<div class='alert alert-success'>\n",
    "    <h1 align=\"center\"> Digit Recognizer with Multi-class logistic regression & Softmax classifier & Neural Networks</h1> \n",
    "     \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQuq-ifvmgHZ"
   },
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVKACykhmgHa"
   },
   "source": [
    "- Multi-class logistic regression\n",
    "- Softmax classifier\n",
    "- Neural Networks\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HtaNZF9pmgHd"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplot_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     13\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "import gzip, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "from plot_utils import *\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyMNv1m7mgHe"
   },
   "outputs": [],
   "source": [
    "# matplotlib configuration\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['image.interpolation'] = 'spline16'\n",
    "\n",
    "# numpy setup\n",
    "np.set_printoptions(precision=2)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1dKNvL4mgIP"
   },
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44lUSvC8mgIU",
    "outputId": "eca54d46-2b8c-4998-9656-91cf82f8b38a"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/mnist.pkl.gz'\n",
    "\n",
    "with gzip.open(DATA_PATH, 'rb') as f:\n",
    "    (X_train, y_train), (X_valid, y_valid), (X_test,  y_test) = pickle.load(f, encoding='latin1')\n",
    "\n",
    "# As a sanity check, we print out the size of the data.\n",
    "print('Training data shape:    ', X_train.shape)\n",
    "print('Training labels shape:  ', y_train.shape)\n",
    "print('Validation data shape:  ', X_valid.shape)\n",
    "print('Validation labels shape:', y_valid.shape)\n",
    "print('Test data shape:        ', X_test.shape)\n",
    "print('Test labels shape:      ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ATcbDzdmgIW"
   },
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0K-YnASJmgIW"
   },
   "outputs": [],
   "source": [
    "classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3Ob2UZBmgIX",
    "outputId": "ae9ac2a0-3410-446d-d24e-faa4b00d8c28"
   },
   "outputs": [],
   "source": [
    "plot_random_samples(X_train, y_train, classes, samples_per_class=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkioJaIGmgIX",
    "outputId": "f83c3338-0b37-4f90-97d5-db33d59dcec1"
   },
   "outputs": [],
   "source": [
    "plot_sample(X_train, y_train, annot=True, idx=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DspOlK5qmgIX"
   },
   "source": [
    "### Preprocessing: normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VewsLJgzmgIY"
   },
   "outputs": [],
   "source": [
    "# compute mean vector from training data\n",
    "mu = np.mean(X_train, axis=0)\n",
    "\n",
    "# remove mean vector from all data\n",
    "X_train -= mu\n",
    "X_valid -= mu\n",
    "X_test  -= mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeuI2s9omgIY"
   },
   "source": [
    "Plotting the mean vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWCnByK3mgIZ",
    "outputId": "30f27734-0619-496d-d381-5ddb2f3b6e46"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(mu.reshape(28, 28), interpolation='nearest', cmap=plt.cm.Greys)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Mean value of features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0R2T7glmgIZ"
   },
   "source": [
    "## Multi-Class Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3KBs60VmgIb"
   },
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zb4e660KmgIc"
   },
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "    scores = X @ W + b\n",
    "    return np.argmax(scores, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGWZMG1WmgIc"
   },
   "source": [
    "#### Prediction for random parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onslsJ68mgIc"
   },
   "source": [
    "Since here we have only 10 different classes, if we predict labels using random initial weights and biases, we must get an accuracy about 10 percent ($\\frac{1}{10}$). This means that the classifier is guessing the labels totally random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0E2I1C_GmgIc"
   },
   "outputs": [],
   "source": [
    "c = 10                # number of classes \n",
    "n = X_train.shape[1]  # number of features\n",
    "\n",
    "# init parameters randomly\n",
    "W = 0.01 * np.random.randn(n, c)\n",
    "b = np.zeros(c)\n",
    "\n",
    "# predict classes and compute accuracy\n",
    "y_pred = predict(W, b, X_train)\n",
    "acc = accuracy(y_pred, y_train)  # this function is defined in utils.py\n",
    "print(\"Accuracy = {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCi1o0TpmgIc"
   },
   "source": [
    "## Softmax classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9nqZwrAmgId"
   },
   "source": [
    "in which, $y^{(i)}$ is the correct class for input $x^{(i)}$ and $s_j$ is the computed score for class $j$ for $j \\in \\{1, 2, \\dots, c\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "os_cvCkbmgIg"
   },
   "source": [
    "### Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlrkregqmgIg"
   },
   "outputs": [],
   "source": [
    "def softmax_loss(W, b, X_batch, y_batch, mode='train'):\n",
    "    bs = X_batch.shape[0]  # batch size\n",
    "    \n",
    "    scores = X_batch @ W + b\n",
    "    probs = softmax(scores)\n",
    "    loss = -np.sum(np.log(probs[range(bs), y_batch])) / bs\n",
    "    \n",
    "    if mode == 'test':\n",
    "        return loss\n",
    "    \n",
    "    # compute gradients w.r.t scores\n",
    "    dscores = np.copy(probs)\n",
    "    dscores[range(bs), y_batch] -= 1.0\n",
    "    dscores /= bs\n",
    "    \n",
    "    # compute gradients w.r.t W and b\n",
    "    db = dscores.sum(axis=0)\n",
    "    dW = X_batch.T @ dscores\n",
    "    \n",
    "    return loss, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RfRgKrwmgIg"
   },
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X_train, y_train, X_valid, y_valid, batch_size=32, \n",
    "                                alpha=0.01, lmbda=1e-4, num_epochs=100):\n",
    "    \n",
    "    m, n = X_train.shape\n",
    "    num_batches = m % batch_size\n",
    "    \n",
    "    report = \"Epoch {:3d}: training loss = {:.2f} | validation loss = {:.2f}\"\n",
    "    \n",
    "    # init parameters randomly\n",
    "    W = np.random.randn(n, 10) * 0.001\n",
    "    b = np.zeros((10,))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            \n",
    "            # select a random mini-batch\n",
    "            idx = np.random.choice(m, batch_size, replace=False)\n",
    "            X_batch, y_batch = X_train[idx], y_train[idx]\n",
    "            \n",
    "            # compute loss and gradient\n",
    "            loss, dW, db = softmax_loss(W, b, X_batch, y_batch)  # data loss\n",
    "            loss += 0.5 * lmbda * np.sum(W ** 2)                 # regularization loss\n",
    "            dW += lmbda * W\n",
    "            \n",
    "            train_loss += loss\n",
    "            \n",
    "            # update parameters            \n",
    "            b = b - alpha * db\n",
    "            W = W - alpha * dW\n",
    "        \n",
    "        # report stats after each epoch\n",
    "        train_loss /= num_batches        \n",
    "        valid_loss = softmax_loss(W, b, X_valid, y_valid, mode='test')\n",
    "        print(report.format(epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHSe1GummgIh",
    "outputId": "c97e2719-1a89-4fbe-c590-dac6dbd95a94"
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "alpha = 1e-2\n",
    "lmbda = 1e-4\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "\n",
    "# run mini-batch gradient descent\n",
    "W, b = mini_batch_gradient_descent(X_train, y_train, X_valid, y_valid, \n",
    "                                   batch_size=batch_size, alpha=alpha,\n",
    "                                   lmbda=lmbda, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxKzaHcNmgIh",
    "outputId": "ab9fe592-d389-48c3-cc82-fca3f32b8daa"
   },
   "outputs": [],
   "source": [
    "train_acc = accuracy(predict(W, b, X_train), y_train)\n",
    "valid_acc = accuracy(predict(W, b, X_valid), y_valid)\n",
    "\n",
    "print('Training accuracy =   {:.2f}%'.format(train_acc))\n",
    "print('Validation accuracy = {:.2f}%'.format(valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFjUPzbEmgIi"
   },
   "source": [
    "#### Prediction for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOavAiQfmgIi",
    "outputId": "8ea2eef3-bd31-44d2-87d5-2303ac0f4674"
   },
   "outputs": [],
   "source": [
    "test_acc = accuracy(predict(W, b, X_test), y_test)\n",
    "print('Test accuracy =   {:.2f}%'.format(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4C5Fo8_mmgIu",
    "outputId": "4fbb7e2d-eb60-4a13-f87f-4119d2de7773",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    idx = np.random.choice(X_test.shape[0])\n",
    "    probs = softmax(X_test[idx].reshape((1, -1)) @ W + b)[0]\n",
    "    predict_and_plot(probs, X_test[idx], y_test[idx], mu, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNYsP2uGmgIu"
   },
   "source": [
    "### Visualizing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCIIeHbZmgIu"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(W[:, i].reshape((28, 28)), cmap=plt.cm.coolwarm)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"%d\" % i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32rvXEQOmgIu"
   },
   "source": [
    "### Visualizing data in 2D space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVjzKbWZmgIv"
   },
   "source": [
    "**t-SNE:** ([paper](http://www.cs.toronto.edu/~hinton/absps/tsne.pdf))\n",
    "- a technique for **dimensionality reduction**.\n",
    "- particularly well suited for visualizing high-dimensional datasets in 2D or 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "lpWysJcimgIv",
    "outputId": "db61fc03-1755-4afc-f19f-cf05fc688613"
   },
   "outputs": [],
   "source": [
    "plot_tsne(X_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAQIdJv3mgIv"
   },
   "source": [
    "## Non-linear Classification: Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0z9O4yKDmgIv"
   },
   "outputs": [],
   "source": [
    "from utils import softmax_loss\n",
    "\n",
    "\n",
    "class TwoLayerNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, num_features=784, num_hiddens=20, num_classes=10):\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # random initialization: create random weights, set all biases to zero\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(num_features, num_hiddens) * 0.001\n",
    "        self.params['W2'] = np.random.randn(num_hiddens,  num_classes) * 0.001\n",
    "        self.params['b1'] = np.zeros((num_hiddens,))\n",
    "        self.params['b2'] = np.zeros((num_classes,))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # forward step\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        \n",
    "        # forward step\n",
    "        h_in = X @ W1 + b1       # hidden layer input\n",
    "        h = np.maximum(0, h_in)  # hidden layer output (using ReLU)\n",
    "        scores = h @ W2 + b2     # neural net output\n",
    "        \n",
    "        return scores\n",
    "                            \n",
    "    def train_step(self, X, y):\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        \n",
    "        # forward step\n",
    "        h_in = X @ W1 + b1       # hidden layer input\n",
    "        h = np.maximum(0, h_in)  # hidden layer output (using ReLU)\n",
    "        scores = h @ W2 + b2     # neural net output\n",
    "        \n",
    "        # compute loss\n",
    "        loss, dscores = softmax_loss(scores, y)\n",
    "        \n",
    "        # backward step\n",
    "        db2 = dscores.sum(axis=0)\n",
    "        dW2 = h.T @ dscores\n",
    "        \n",
    "        dh = dscores @ W2.T\n",
    "        dh[h_in < 0] = 0.0\n",
    "        db1 = dh.sum(axis=0)\n",
    "        dW1 = X.T @ dh\n",
    "        \n",
    "        gradient = {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}\n",
    "                \n",
    "        return loss, gradient\n",
    "        \n",
    "    def train(self, X_train, y_train, X_valid, y_valid, batch_size=50, \n",
    "              alpha=0.001, lmbda=0.0001, num_epochs=10):\n",
    "        \n",
    "        m, n = X_train.shape        \n",
    "        num_batches = m // batch_size\n",
    "        \n",
    "        report = \"{:3d}: training loss = {:.2f} | validation loss = {:.2f}\"\n",
    "        \n",
    "        losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for _ in range(num_batches):\n",
    "                W1, b1 = self.params['W1'], self.params['b1']\n",
    "                W2, b2 = self.params['W2'], self.params['b2']\n",
    "                \n",
    "                # select a random mini-batch\n",
    "                batch_idx = np.random.choice(m, batch_size, replace=False)\n",
    "                X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
    "\n",
    "                # train on mini-batch\n",
    "                data_loss, gradient = self.train_step(X_batch, y_batch)\n",
    "                reg_loss = 0.5 * (np.sum(W1 ** 2) + np.sum(W2 ** 2))\n",
    "                train_loss += (data_loss + lmbda * reg_loss)\n",
    "                losses.append(data_loss + lmbda * reg_loss)\n",
    "\n",
    "                # regularization\n",
    "                gradient['W1'] += lmbda * W1\n",
    "                gradient['W2'] += lmbda * W2\n",
    "\n",
    "                # update parameters\n",
    "                for p in self.params:\n",
    "                    self.params[p] = self.params[p] - alpha * gradient[p]\n",
    "            \n",
    "            # report training loss and validation loss\n",
    "            train_loss /= num_batches\n",
    "            valid_loss = softmax_loss(self.forward(X_valid), y_valid, mode='test')\n",
    "            print(report.format(epoch + 1, train_loss, valid_loss))\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict labels for input data.\n",
    "        \"\"\"\n",
    "        scores = self.forward(X)\n",
    "        return np.argmax(scores, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Predict probabilties of classes for each input data.\n",
    "        \"\"\"\n",
    "        scores = self.forward(X)\n",
    "        return softmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33yGekZ2mgIw"
   },
   "outputs": [],
   "source": [
    "mlp = TwoLayerNeuralNetwork(num_hiddens=20)\n",
    "losses = mlp.train(X_train, y_train, X_valid, y_valid, \n",
    "                   alpha=0.05, lmbda=0.001, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ke5yoVkFmgIw"
   },
   "outputs": [],
   "source": [
    "train_acc = accuracy(mlp.predict(X_train), y_train)\n",
    "print(\"Training accuracy   = {:.2f}%\".format(train_acc))\n",
    "\n",
    "test_acc = accuracy(mlp.predict(X_test), y_test)\n",
    "print(\"Validation accuracy = {:.2f}%\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTMqRFfwmgIw"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(range(0, 10001, 1000), range(0, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR_zI1-UmgIw"
   },
   "source": [
    "### Prediction for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itxwUHmimgIw"
   },
   "outputs": [],
   "source": [
    "test_acc = accuracy(predict(W, b, X_test), y_test)\n",
    "print('Test accuracy = {:.2f}%'.format(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wygQ8buPmgIx",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    idx = np.random.choice(len(y_test))\n",
    "    probs = mlp.predict_proba(X_test[idx].reshape((1, -1)))[0]\n",
    "    predict_and_plot(probs, X_test[idx], y_test[idx], mu, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nY1TBvNDmgIx"
   },
   "source": [
    "### Plot misclassified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFq0jdaOmgIx",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = 10\n",
    "y_pred = mlp.predict(X_test)\n",
    "idx = np.flatnonzero(y_pred != y_test)\n",
    "sample_idx = np.random.choice(idx, count)\n",
    "\n",
    "for i in sample_idx:\n",
    "    probs = mlp.predict_proba(X_test[i].reshape((1, -1)))[0]\n",
    "    predict_and_plot(probs, X_test[i], y_test[i], mu, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxroYCRVmgIx"
   },
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkGunxepmgIx"
   },
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test)\n",
    "plot_confusion_matrix(y_test, y_pred, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG6pND2cmgIx"
   },
   "source": [
    "### Visualizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lM_5oLqKmgIx"
   },
   "outputs": [],
   "source": [
    "W = mlp.params['W1']\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FUDfWQDmgIx"
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(W[:, i].reshape((28, 28)), cmap=plt.cm.coolwarm)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quOlmh8YmgIx"
   },
   "source": [
    "## Training Using Advanced Optimization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLA6a2XnmgIy"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9KEavcpmgIy"
   },
   "outputs": [],
   "source": [
    "def neural_net_loss(W, X, y, lmbda=0.001, F=784, H=20, C=10, mode='train'):\n",
    "    \n",
    "    # extract W1, b1, W2, b2 from W\n",
    "    W1 = np.reshape(W[: F * H], (F, H))\n",
    "    b1 = W[F * H: (F + 1) * H]\n",
    "    \n",
    "    W2 = np.reshape(W[(F + 1) * H: (F + 1) * H + H * C], (H, C))\n",
    "    b2 = W[(F + 1) * H + H * C:]\n",
    "    \n",
    "    # Forward step\n",
    "    h_in = X @ W1 + b1\n",
    "    h = np.maximum(0, h_in)\n",
    "    scores = h @ W2 + b2\n",
    "        \n",
    "    # compute loss\n",
    "    if mode != 'train':\n",
    "        return softmax_loss(scores, y, mode=mode)\n",
    "    \n",
    "    loss, dscores = softmax_loss(scores, y)\n",
    "    loss += 0.5 * lmbda * (np.sum(W1 ** 2) + np.sum(W2 ** 2))\n",
    "    \n",
    "    # Backward Step\n",
    "    db2 = dscores.sum(axis=0)\n",
    "    dW2 = h.T @ dscores\n",
    "    dh = dscores @ W2.T\n",
    "    \n",
    "    dh[h_in < 0] = 0.0\n",
    "    db1 = dh.sum(axis=0)\n",
    "    dW1 = X.T @ dh\n",
    "    \n",
    "    dW1 += lmbda * W1\n",
    "    dW2 += lmbda * W2\n",
    "    \n",
    "    # concatenate all grads in a column vector\n",
    "    dW = np.concatenate((dW1.ravel(), db1, dW2.ravel(), db2), axis=0)\n",
    "    \n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmKJBlAcmgIy"
   },
   "outputs": [],
   "source": [
    "def report_callback(W):\n",
    "    loss = neural_net_loss(W, X_valid, y_valid, mode='test')\n",
    "    print(\"Validation loss = {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nG1Im4LLmgIy"
   },
   "outputs": [],
   "source": [
    "def predict_nn(W1, b1, W2, b2, X):\n",
    "    h = np.maximum(0, X @ W1 + b1)\n",
    "    scores = h @ W2 + b2\n",
    "    return np.argmax(scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahfgBGqqmgI0"
   },
   "outputs": [],
   "source": [
    "F = 784  # num. features\n",
    "H = 20   # num. hidden neurons\n",
    "C = 10   # num. classes\n",
    "\n",
    "# create and init parameters W1, W2\n",
    "W1 = np.random.randn(F, H) * 0.001\n",
    "W2 = np.random.randn(H, C) * 0.001\n",
    "\n",
    "b1 = np.zeros((H,))\n",
    "b2 = np.zeros((C,))\n",
    "\n",
    "# concat all parameters in one column vector\n",
    "W = np.concatenate((W1.ravel(), b1, W2.ravel(), b2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PSHCWigmgI0"
   },
   "outputs": [],
   "source": [
    "result = minimize(\n",
    "    neural_net_loss, \n",
    "    x0=W, \n",
    "    args=(X_train, y_train), \n",
    "    method='CG', \n",
    "    jac=True,\n",
    "    options={'maxiter': 50},\n",
    "    callback=report_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLk9PipQmgI0"
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCZIAOvImgI1"
   },
   "outputs": [],
   "source": [
    "W = result.x\n",
    "\n",
    "# extract W1, b1, W2, b2 from W\n",
    "W1 = np.reshape(W[: F * H], (F, H))\n",
    "b1 = W[F * H: (F + 1) * H]\n",
    "\n",
    "W2 = np.reshape(W[(F + 1) * H: (F + 1) * H + H * C], (H, C))\n",
    "b2 = W[(F + 1) * H + H * C:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpCi5UZfmgI1"
   },
   "outputs": [],
   "source": [
    "acc = accuracy(predict_nn(W1, b1, W2, b2, X_test), y_test)\n",
    "print(\"Test accuracy = {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3emtKTemgI1"
   },
   "source": [
    "### Neural networks in scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IArU72ODmgI1"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWQEHfOomgI1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(20,), learning_rate='adaptive', alpha=1.0, max_iter=50, verbose=1)\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BSa58EumgI1"
   },
   "outputs": [],
   "source": [
    "train_acc = model.score(X_train, y_train)\n",
    "print(\"Train accuracy = {:.2f}%\".format(train_acc * 100))\n",
    "\n",
    "test_acc = model.score(X_test, y_test)\n",
    "print(\"Test accuracy  = {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "6-neural_networks_demo-digits-MNIST.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "6fd24dc732891b5f87b0cf4d9c4a9ecb93a25a51929f0934071c5bb9199a7603"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
